{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import emcee\n",
        "from scipy.stats import beta\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8btnVdViqULo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Estimation of the DLM model\n",
        "## **$ $**\n",
        "<h2><strong>\\( Y_t = F_t \\theta_t + X_t \\eta + Z_t \\zeta + \\upsilon_t \\)</strong></h2>\n",
        "<h2><strong>\\( \\theta_t = G\\theta_{t-1} + Z_{t-1} \\gamma + \\omega_t \\)</strong></h2>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gJjG1uJYuMGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Estimation procedure using GD**"
      ],
      "metadata": {
        "id": "TZSphAxrs0yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the negative log-likelihood function\n",
        "def negative_log_likelihood(params, Y_t, X_t, Z_t):\n",
        "    G, *coeffs = params\n",
        "    eta = np.array(coeffs[:X_t.shape[1]])\n",
        "    zeta = np.array(coeffs[X_t.shape[1]:])\n",
        "\n",
        "    T = len(Y_t)\n",
        "    theta_t = np.zeros(T)\n",
        "    neg_log_likelihood = 0\n",
        "\n",
        "    for t in range(T):\n",
        "        if t > 0:\n",
        "            theta_t[t] = G * theta_t[t-1] + np.dot(Z_t[t-1], zeta / 2)\n",
        "\n",
        "        predicted_Y_t = theta_t[t] + np.dot(X_t[t], eta) + np.dot(Z_t[t], zeta / 2)\n",
        "        neg_log_likelihood += 0.5 * ((Y_t[t] - predicted_Y_t)**2)\n",
        "    return neg_log_likelihood\n",
        "\n",
        "# Gradient descent implementation\n",
        "def estimation_gradient_descent(Y_t, X_t, Z_t, initial_params, learning_rate=0.001, n_iterations=500):\n",
        "    params = np.array(initial_params)\n",
        "    n_params = len(params)\n",
        "\n",
        "    for iteration in range(n_iterations):\n",
        "        gradients = np.zeros(n_params)\n",
        "\n",
        "        # Compute gradients\n",
        "        for i in range(n_params):\n",
        "            original_param = params[i]\n",
        "\n",
        "            # Compute loss at original parameter\n",
        "            loss_original = negative_log_likelihood(params, Y_t, X_t, Z_t)\n",
        "\n",
        "            # Perturb parameter\n",
        "            params[i] = original_param + 1e-5\n",
        "            loss_perturbed = negative_log_likelihood(params, Y_t, X_t, Z_t)\n",
        "\n",
        "            # Compute gradient\n",
        "            gradients[i] = (loss_perturbed - loss_original) / 1e-5\n",
        "\n",
        "            # Reset parameter\n",
        "            params[i] = original_param\n",
        "\n",
        "        # Update parameters\n",
        "        params -= learning_rate * gradients\n",
        "\n",
        "        if iteration % 10 == 0:\n",
        "            print(f\"Iteration {iteration}: Loss = {loss_original}\")\n",
        "            if np.isnan(loss_original):\n",
        "                print(\"NaN encountered in loss. Exiting.\")\n",
        "                break\n",
        "\n",
        "    return params"
      ],
      "metadata": {
        "id": "RsMalJ5ostXx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}